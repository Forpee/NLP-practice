{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "draft_tf_tut_0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq7nv4ao9oBB85OoG4CKBG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Forpee/NLP-practice/blob/master/text_generation/draft_tf_tut_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation with rnn"
      ],
      "metadata": {
        "id": "hGjdJao9lHIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Get data\n",
        "2. Read data\n",
        "3. Split text into chars\n",
        "4. Convert chars to id's\n",
        "5. Create a way to get text from id's\n",
        "6. Create dataset from id's\n",
        "7. Batch dataset\n",
        "8. Shift all chars in dataset by one to get labels\n",
        "9. Improve performance on dataset\n",
        "10. Build the model\n",
        "\n"
      ],
      "metadata": {
        "id": "uqY0aj5aVZWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. \n",
        "\n",
        "This makes the task a classification problem.\n",
        "Which char (class) has the highest probability of being next"
      ],
      "metadata": {
        "id": "OJdn9ZzHZl6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "d0TFSCLrUHYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "lbMAIwMOldhW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get data"
      ],
      "metadata": {
        "id": "6nx9s7HRUK2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file=tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "6RN_esELmwx4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Examine data"
      ],
      "metadata": {
        "id": "vbBZJEEtUPbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'{len(text)}')"
      ],
      "metadata": {
        "id": "47herT-inJED",
        "outputId": "4c558f67-b9ad-40b8-9f30-087c21dedf63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "id": "dDJjz1M9n17U",
        "outputId": "1bd18b5f-0e08-477c-93b6-2227e8b1826f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "VHO5Z0hmoM8_",
        "outputId": "4791b7c5-a646-4263-c43b-7a04a976863f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process the text"
      ],
      "metadata": {
        "id": "FdMFmWXPo-Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H9ArLNlKZHf",
        "outputId": "986d4554-1737-4493-b15d-29e690191806"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "Cf89P2ZtLe4x"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert from numbers back to chars"
      ],
      "metadata": {
        "id": "mw2smbUcL8qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "Agq7YhASKs-m"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can tf.strings.reduce_join to join the characters back into strings."
      ],
      "metadata": {
        "id": "xy1P3DIDLRbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "jlO8NPJaLPTq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNmMQNBdLXF-",
        "outputId": "85a92735-fc4c-4586-b713-9aa22c2925eb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "f530eIhOL034"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYTvWufPL7DU",
        "outputId": "3a9af447-9857-4f82-a3b4-1e6369b61f89"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "YwwJtq3fMG42"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The batch method lets you easily convert these individual characters to sequences of the desired size."
      ],
      "metadata": {
        "id": "bjo6ThuRMWaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wArF28xMb-r",
        "outputId": "8f465137-d868-4b4b-daff-d20def65b710"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_RD_2-CMf5w",
        "outputId": "bc6ece8d-a8ba-4acc-eeb4-cea6af310995"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "v96mBSzUMzEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "y_gL21YwM014"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J803Xm7BM2Qw",
        "outputId": "e8d845ed-4e93-4865-e5a8-bbf7641888ec"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "Rc7OHG_1NA04"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzmt-tu6NEc4",
        "outputId": "9a6444b8-3471-43cf-adbe-7661257c7a35"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create training batches"
      ],
      "metadata": {
        "id": "M0sw2ZzhNLd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcNxt3RHNM3j",
        "outputId": "43b93821-17d9-46fc-b366-2a7eee220c96"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the model"
      ],
      "metadata": {
        "id": "5dHvzZb3NWWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "wCyaloIxNY9l"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru=tf.keras.layers.GRU(rnn_units,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True)\n",
        "    self.dense=tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x=inputs\n",
        "    x=self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states,training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "aUwzLwSlNo69"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to return the previous states so the model has some context as to what to predict next"
      ],
      "metadata": {
        "id": "zhDLa60cc8dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "Ej_gmLAIdUwb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "aJzkBSh-dvOK"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "B1FsXOdadxFH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "checkpoints"
      ],
      "metadata": {
        "id": "uq5BwFwad3sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "FUmIIY-Zd47x"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "ur3hdrh1d63H"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHBD5bp5d-_v",
        "outputId": "5f474c6f-1484-40a7-ed90-de40ecdef5cc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 57ms/step - loss: 2.7323\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.9986\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.7207\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.5583\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 60ms/step - loss: 1.4587\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.3902\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3369\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2925\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 1.2519\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.2129\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1740\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1339\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 1.0916\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 1.0483\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 60ms/step - loss: 1.0010\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.9524\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 58ms/step - loss: 0.9008\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8492\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 59ms/step - loss: 0.7972\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.7472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_char=tf.constant(['ROMEO:'])\n",
        "states=None\n",
        "result = [next_char]"
      ],
      "metadata": {
        "id": "Aq4zrEGDiGBY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(1000):\n",
        "  input_chars = tf.strings.unicode_split(next_char, 'UTF-8')\n",
        "  input_ids = ids_from_chars(input_chars).to_tensor()\n",
        "  predicted_logits, states=model(inputs=input_ids, return_state=True, states=states)\n",
        "  predicted_logits = predicted_logits[:, -1, :]\n",
        "  predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "  predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "  next_char = chars_from_ids(predicted_ids)\n",
        "  result.append(next_char)\n"
      ],
      "metadata": {
        "id": "7o_1LQColAGf"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "id": "Mcvv3olul1OE",
        "outputId": "16323fbc-8156-4d30-c3b3-67be53c26bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Taste of some other house that goes to leave us.\n",
            "\n",
            "GLOUCESTER:\n",
            "What fitter that is bannel's faney till he have came.\n",
            "\n",
            "GLOUCESTER:\n",
            "It is your voices?\n",
            "\n",
            "TRANIO:\n",
            "I say shall find my veary like a misbear.\n",
            "\n",
            "VOLUMNIA:\n",
            "You would this bridegral that to mine untaughts:\n",
            "The runnows and honour of himself,\n",
            "Our children have as sorrow's eye:\n",
            "And therefore comes the humble seating\n",
            "Matters of great to orroy allied: dear met,\n",
            "And stop at lamentations, from this fashion,\n",
            "You, gentle Capulet. My master,\n",
            "Not to fall blask us to defany home\n",
            "Are nursed by bowl'd in it all down yet:\n",
            "But 'twill attend a brace or command.\n",
            "\n",
            "POLIXENES:\n",
            "\n",
            "Third Watchman:\n",
            "Even here unto a pedgival?\n",
            "\n",
            "ANGELO:\n",
            "Good morrow; good my lord.\n",
            "\n",
            "PETRUCHIO:\n",
            "Ay, I most creaple, he was brought thee in my heart,\n",
            "Or, if not breathing to recorce before the\n",
            "very father, and the town of Herrely.\n",
            "\n",
            "HASTINGS:\n",
            "My Lord of Gloucester, in God's natural;\n",
            "For she look to the queen your words, let hell be so\n",
            "qualified in bittern present age.\n",
            "Where is the poopl \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}