{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "draft_tf_tut_rap.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Forpee/NLP-practice/blob/master/text_generation/draft_tf_tut_rap.ipynb",
      "authorship_tag": "ABX9TyNa64se/gwS98/UdCj1vGF6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Forpee/NLP-practice/blob/master/text_generation/draft_tf_tut_rap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text generation"
      ],
      "metadata": {
        "id": "b4_gFj3Gy82G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "v_ef1j0n6KWv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxpGHQnNzNyE",
        "outputId": "6b2aab1e-0cdf-4f57-ef49-45ff45520685"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-2bb19a8f-a55d-3608-0446-1e466482e094)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/Tf'"
      ],
      "metadata": {
        "id": "klo69glmzTeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc211f69-1ec0-449a-9037-f3083e27bf16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Binance_ICPUSDT_1h.csv   Binance_ICPUSDT_1h.gsheet  'SONG LYRICS.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Tf/SONG LYRICS.zip' -d '/content/data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONjf-c1u4SGg",
        "outputId": "ebfa0249-d18f-4dae-fd02-79a4ebd47c5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Tf/SONG LYRICS.zip\n",
            "replace /content/data/Kanye_West.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/data/Kanye_West.txt  \n",
            "  inflating: /content/data/Lil_Wayne.txt  \n",
            "  inflating: /content/data/adele.txt  \n",
            "  inflating: /content/data/al-green.txt  \n",
            "  inflating: /content/data/alicia-keys.txt  \n",
            "  inflating: /content/data/amy-winehouse.txt  \n",
            "  inflating: /content/data/beatles.txt  \n",
            "  inflating: /content/data/bieber.txt  \n",
            "  inflating: /content/data/bjork.txt  \n",
            "  inflating: /content/data/blink-182.txt  \n",
            "  inflating: /content/data/bob-dylan.txt  \n",
            "  inflating: /content/data/bob-marley.txt  \n",
            "  inflating: /content/data/britney-spears.txt  \n",
            "  inflating: /content/data/bruce-springsteen.txt  \n",
            "  inflating: /content/data/bruno-mars.txt  \n",
            "  inflating: /content/data/cake.txt  \n",
            "  inflating: /content/data/dickinson.txt  \n",
            "  inflating: /content/data/disney.txt  \n",
            "  inflating: /content/data/dj-khaled.txt  \n",
            "  inflating: /content/data/dolly-parton.txt  \n",
            "  inflating: /content/data/dr-seuss.txt  \n",
            "  inflating: /content/data/drake.txt  \n",
            "  inflating: /content/data/eminem.txt  \n",
            "  inflating: /content/data/janisjoplin.txt  \n",
            "  inflating: /content/data/jimi-hendrix.txt  \n",
            "  inflating: /content/data/johnny-cash.txt  \n",
            "  inflating: /content/data/joni-mitchell.txt  \n",
            "  inflating: /content/data/kanye-west.txt  \n",
            "  inflating: /content/data/kanye.txt  \n",
            "  inflating: /content/data/lady-gaga.txt  \n",
            "  inflating: /content/data/leonard-cohen.txt  \n",
            "  inflating: /content/data/lil-wayne.txt  \n",
            "  inflating: /content/data/lin-manuel-miranda.txt  \n",
            "  inflating: /content/data/lorde.txt  \n",
            "  inflating: /content/data/ludacris.txt  \n",
            "  inflating: /content/data/michael-jackson.txt  \n",
            "  inflating: /content/data/missy-elliott.txt  \n",
            "  inflating: /content/data/nickelback.txt  \n",
            "  inflating: /content/data/nicki-minaj.txt  \n",
            "  inflating: /content/data/nirvana.txt  \n",
            "  inflating: /content/data/notorious-big.txt  \n",
            "  inflating: /content/data/notorious_big.txt  \n",
            "  inflating: /content/data/nursery_rhymes.txt  \n",
            "  inflating: /content/data/patti-smith.txt  \n",
            "  inflating: /content/data/paul-simon.txt  \n",
            "  inflating: /content/data/prince.txt  \n",
            "  inflating: /content/data/r-kelly.txt  \n",
            "  inflating: /content/data/radiohead.txt  \n",
            "  inflating: /content/data/rihanna.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= open('/content/data/lil-wayne.txt', 'rb').read().decode(encoding='utf-8')\n",
        "print(f'{len(text)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35Ik4nwb4j-W",
        "outputId": "6841e49d-e144-4fac-b648-9b80fb3e260b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[250:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V_rzJLy5tL7",
        "outputId": "e5f949ff-a290-4384-bcf7-9e97f60bd637"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o get a whole key i bought my frist key my first key\r\n",
            "Inand we was getting em like for twenty five\r\n",
            "Colombian connect homey we was getting fly\r\n",
            "We on the grind and our nuts got bigga\r\n",
            "And every day we in the motherfucking hood our guns got bigga\r\n",
            "Stu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgmMKnLd591F",
        "outputId": "3a78573b-019a-49ee-c09e-da39e5cd4996"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "eb39f2w46Ihw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "bbkETDPU6I8Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "4cCJkaaH6Xfl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB0wUQDV6bjy",
        "outputId": "f993afbe-dd75-46b3-e70e-0305bcb5e4c9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(117289,), dtype=int64, numpy=array([13,  3, 32, ...,  1,  2,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "0xIUnqmo6deK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCttoUlR6fvy",
        "outputId": "5f8d361d-f672-425c-a7de-506bbac8167d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            " \n",
            "b\n",
            "o\n",
            "u\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "KelaTlvK6h9-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMDdjXLC6mBB",
        "outputId": "7307ef1d-167d-4f9d-e750-0ad55ef93655"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'I' b' ' b'b' b'o' b'u' b'g' b'h' b't' b' ' b'm' b'y' b' ' b'f' b'i'\n",
            " b'r' b's' b't' b' ' b'k' b'e' b'y' b' ' b'f' b'r' b'o' b'm' b' ' b'm'\n",
            " b'y' b' ' b'b' b'a' b'b' b'y' b' ' b'm' b'o' b'm' b'm' b'a' b' ' b'b'\n",
            " b'r' b'o' b't' b'h' b'e' b'r' b'\\r' b'\\n' b'I' b' ' b'b' b'o' b'u' b'g'\n",
            " b'h' b't' b' ' b'm' b'y' b' ' b'f' b'i' b'r' b's' b't' b' ' b'k' b'e'\n",
            " b'y' b'\\r' b'\\n' b'B' b'o' b'u' b'g' b'h' b't' b' ' b'm' b'y' b' ' b'b'\n",
            " b'o' b'u' b'g' b'h' b't' b' ' b'm' b'y' b' ' b'f' b'i' b'r' b's' b't'\n",
            " b' ' b'k' b'e'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3WsA8N36oaK",
        "outputId": "9daa6086-cb2a-4412-b321-96bc5bf532ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'I bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first ke'\n",
            "b'y\\r\\nI bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first'\n",
            "b' key\\r\\nYeah hustling on my city streets\\r\\nTrying to get a whole key i bought my frist key my first key\\r'\n",
            "b'\\nInand we was getting em like for twenty five\\r\\nColombian connect homey we was getting fly\\r\\nWe on the '\n",
            "b'grind and our nuts got bigga\\r\\nAnd every day we in the motherfucking hood our guns got bigga\\r\\nStunting'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "  "
      ],
      "metadata": {
        "id": "PnUWJKBB6sAt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoggDMtp6t-B",
        "outputId": "985c1541-59e1-4fc5-9ff6-eb1f4294976f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "CWrOZNM36wK8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dopw49EE6zHX",
        "outputId": "fc15ef44-1116-4abc-bf01-a1ee1da52d4e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'I bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first k'\n",
            "Target: b' bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first ke'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXPAFqEv62sc",
        "outputId": "d73899c7-ca89-4871-b582-25f309ea9de0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "icC0Yk-e66i5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, rnn_units, vocab_size, embedding_dim):\n",
        "    super().__init__(self)\n",
        "    self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru=tf.keras.layers.GRU(rnn_units,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True)\n",
        "    self.dense=tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x=inputs\n",
        "    x=self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x=self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "    "
      ],
      "metadata": {
        "id": "DECQdHVM7UFb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "lM-hNMiu-T8r"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "Glv0Qrkf-jsR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "4Dc9gNtg-lz7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "dnA6t_bO-vgw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40"
      ],
      "metadata": {
        "id": "3OwC-8Re-qeV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "AzyCXwZA-raX",
        "outputId": "6b821046-b882-4e3c-cd37-7190aeef0fb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "18/18 [==============================] - 10s 65ms/step - loss: 3.9818\n",
            "Epoch 2/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 3.1264\n",
            "Epoch 3/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 2.7863\n",
            "Epoch 4/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 2.5053\n",
            "Epoch 5/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 2.3527\n",
            "Epoch 6/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 2.2684\n",
            "Epoch 7/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 2.2098\n",
            "Epoch 8/40\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 2.1600\n",
            "Epoch 9/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 2.1091\n",
            "Epoch 10/40\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 2.0589\n",
            "Epoch 11/40\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 2.0104\n",
            "Epoch 12/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.9618\n",
            "Epoch 13/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.9165\n",
            "Epoch 14/40\n",
            "18/18 [==============================] - 1s 64ms/step - loss: 1.8752\n",
            "Epoch 15/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.8311\n",
            "Epoch 16/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.7884\n",
            "Epoch 17/40\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 1.7470\n",
            "Epoch 18/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.7028\n",
            "Epoch 19/40\n",
            "18/18 [==============================] - 1s 67ms/step - loss: 1.6616\n",
            "Epoch 20/40\n",
            "18/18 [==============================] - 1s 65ms/step - loss: 1.6182\n",
            "Epoch 21/40\n",
            "18/18 [==============================] - 1s 65ms/step - loss: 1.5805\n",
            "Epoch 22/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.5400\n",
            "Epoch 23/40\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 1.5022\n",
            "Epoch 24/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.4617\n",
            "Epoch 25/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.4188\n",
            "Epoch 26/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.3761\n",
            "Epoch 27/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.3301\n",
            "Epoch 28/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.2836\n",
            "Epoch 29/40\n",
            "18/18 [==============================] - 1s 62ms/step - loss: 1.2336\n",
            "Epoch 30/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.1819\n",
            "Epoch 31/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.1259\n",
            "Epoch 32/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 1.0714\n",
            "Epoch 33/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 1.0146\n",
            "Epoch 34/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.9520\n",
            "Epoch 35/40\n",
            "18/18 [==============================] - 1s 61ms/step - loss: 0.8838\n",
            "Epoch 36/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.8147\n",
            "Epoch 37/40\n",
            "18/18 [==============================] - 1s 60ms/step - loss: 0.7465\n",
            "Epoch 38/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.6786\n",
            "Epoch 39/40\n",
            "18/18 [==============================] - 1s 59ms/step - loss: 0.6055\n",
            "Epoch 40/40\n",
            "18/18 [==============================] - 1s 63ms/step - loss: 0.5393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_char=tf.constant(['I am'])\n",
        "states=None\n",
        "result = [next_char]"
      ],
      "metadata": {
        "id": "5Xosr_MC-1Wt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(1000):\n",
        "  # Convert strings to token IDs.\n",
        "  input_chars = tf.strings.unicode_split(next_char, 'UTF-8')\n",
        "  input_ids = ids_from_chars(input_chars).to_tensor()\n",
        "  # Run the model.\n",
        "  # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "  predicted_logits, states=model(inputs=input_ids, return_state=True, states=states)\n",
        "  # Only use the last prediction.\n",
        "  predicted_logits = predicted_logits[:, -1, :]\n",
        "  # Sample the output logits to generate token IDs.\n",
        "  predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "  predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "  # Convert from token ids to characters\n",
        "  next_char = chars_from_ids(predicted_ids)\n",
        "  result.append(next_char)"
      ],
      "metadata": {
        "id": "FmNgc_3EAVr6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ],
      "metadata": {
        "id": "rhywl-FjAbFw",
        "outputId": "2cf170ef-99d7-4ae8-c01e-eeceecda35fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am the inger and I aint her I wouldnt\r\n",
            "Mad frimn the you buttin Juck game tight\r\n",
            "And they game from my days on my trieght aint andir but my chick be sixzin\r\n",
            "These niggas cant fucu baby Im fire like a lotees tenging leaven a Tamnes\r\n",
            "Young ways goin drat you already know they car bark off\r\n",
            "Chorus x\r\n",
            "My bodin br sk my check came mover like a baterf coupe and car for your just too neat\r\n",
            "And me bad all findy like as somati\r\n",
            "I just think young money bundin then turnud up in this bitch\r\n",
            "Lets jump off a beag nigga nour Knock DambI\r\n",
            "Gever me she I beat my done leave me want on my mind\r\n",
            "Then we beatin mu\r\n",
            "No not ezery hot go in air\r\n",
            "Like maney graw wo da\r\n",
            "zy plaqeey and I been girl dont play with mive fers apprough\r\n",
            "Its Young Wayne on them hoes\r\n",
            "And whees so Immot mean em floomio flows deact\r\n",
            "And she aint even got a brack to kill a hant to get a fillded baby\r\n",
            "I see it out a windowwin\r\n",
            "Cash Money just loke it out hore do me\r\n",
            "My Chomphes Sooker Tunechi endo bench my grip\r\n",
            "I spil out a windertwen\r\n",
            "Wh \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OlGYrzNCAfP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}