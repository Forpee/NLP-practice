{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "draft_tf_tut_rap.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Forpee/NLP-practice/blob/master/text_generation/draft_tf_tut_rap.ipynb",
      "authorship_tag": "ABX9TyOVXCSf4VMdcnUfEylCycoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Forpee/NLP-practice/blob/master/text_generation/draft_tf_tut_rap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text generation"
      ],
      "metadata": {
        "id": "b4_gFj3Gy82G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "v_ef1j0n6KWv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxpGHQnNzNyE",
        "outputId": "6b2aab1e-0cdf-4f57-ef49-45ff45520685"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-2bb19a8f-a55d-3608-0446-1e466482e094)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/MyDrive/Tf'"
      ],
      "metadata": {
        "id": "klo69glmzTeW",
        "outputId": "dc211f69-1ec0-449a-9037-f3083e27bf16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Binance_ICPUSDT_1h.csv   Binance_ICPUSDT_1h.gsheet  'SONG LYRICS.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Tf/SONG LYRICS.zip' -d '/content/data'"
      ],
      "metadata": {
        "id": "ONjf-c1u4SGg",
        "outputId": "ebfa0249-d18f-4dae-fd02-79a4ebd47c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Tf/SONG LYRICS.zip\n",
            "replace /content/data/Kanye_West.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/data/Kanye_West.txt  \n",
            "  inflating: /content/data/Lil_Wayne.txt  \n",
            "  inflating: /content/data/adele.txt  \n",
            "  inflating: /content/data/al-green.txt  \n",
            "  inflating: /content/data/alicia-keys.txt  \n",
            "  inflating: /content/data/amy-winehouse.txt  \n",
            "  inflating: /content/data/beatles.txt  \n",
            "  inflating: /content/data/bieber.txt  \n",
            "  inflating: /content/data/bjork.txt  \n",
            "  inflating: /content/data/blink-182.txt  \n",
            "  inflating: /content/data/bob-dylan.txt  \n",
            "  inflating: /content/data/bob-marley.txt  \n",
            "  inflating: /content/data/britney-spears.txt  \n",
            "  inflating: /content/data/bruce-springsteen.txt  \n",
            "  inflating: /content/data/bruno-mars.txt  \n",
            "  inflating: /content/data/cake.txt  \n",
            "  inflating: /content/data/dickinson.txt  \n",
            "  inflating: /content/data/disney.txt  \n",
            "  inflating: /content/data/dj-khaled.txt  \n",
            "  inflating: /content/data/dolly-parton.txt  \n",
            "  inflating: /content/data/dr-seuss.txt  \n",
            "  inflating: /content/data/drake.txt  \n",
            "  inflating: /content/data/eminem.txt  \n",
            "  inflating: /content/data/janisjoplin.txt  \n",
            "  inflating: /content/data/jimi-hendrix.txt  \n",
            "  inflating: /content/data/johnny-cash.txt  \n",
            "  inflating: /content/data/joni-mitchell.txt  \n",
            "  inflating: /content/data/kanye-west.txt  \n",
            "  inflating: /content/data/kanye.txt  \n",
            "  inflating: /content/data/lady-gaga.txt  \n",
            "  inflating: /content/data/leonard-cohen.txt  \n",
            "  inflating: /content/data/lil-wayne.txt  \n",
            "  inflating: /content/data/lin-manuel-miranda.txt  \n",
            "  inflating: /content/data/lorde.txt  \n",
            "  inflating: /content/data/ludacris.txt  \n",
            "  inflating: /content/data/michael-jackson.txt  \n",
            "  inflating: /content/data/missy-elliott.txt  \n",
            "  inflating: /content/data/nickelback.txt  \n",
            "  inflating: /content/data/nicki-minaj.txt  \n",
            "  inflating: /content/data/nirvana.txt  \n",
            "  inflating: /content/data/notorious-big.txt  \n",
            "  inflating: /content/data/notorious_big.txt  \n",
            "  inflating: /content/data/nursery_rhymes.txt  \n",
            "  inflating: /content/data/patti-smith.txt  \n",
            "  inflating: /content/data/paul-simon.txt  \n",
            "  inflating: /content/data/prince.txt  \n",
            "  inflating: /content/data/r-kelly.txt  \n",
            "  inflating: /content/data/radiohead.txt  \n",
            "  inflating: /content/data/rihanna.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= open('/content/data/lil-wayne.txt', 'rb').read().decode(encoding='utf-8')\n",
        "print(f'{len(text)}')"
      ],
      "metadata": {
        "id": "35Ik4nwb4j-W",
        "outputId": "6841e49d-e144-4fac-b648-9b80fb3e260b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[250:500])"
      ],
      "metadata": {
        "id": "1V_rzJLy5tL7",
        "outputId": "e5f949ff-a290-4384-bcf7-9e97f60bd637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o get a whole key i bought my frist key my first key\r\n",
            "Inand we was getting em like for twenty five\r\n",
            "Colombian connect homey we was getting fly\r\n",
            "We on the grind and our nuts got bigga\r\n",
            "And every day we in the motherfucking hood our guns got bigga\r\n",
            "Stu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "id": "xgmMKnLd591F",
        "outputId": "3a78573b-019a-49ee-c09e-da39e5cd4996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "eb39f2w46Ihw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "bbkETDPU6I8Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "4cCJkaaH6Xfl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "id": "oB0wUQDV6bjy",
        "outputId": "f993afbe-dd75-46b3-e70e-0305bcb5e4c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(117289,), dtype=int64, numpy=array([13,  3, 32, ...,  1,  2,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "0xIUnqmo6deK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "YCttoUlR6fvy",
        "outputId": "5f8d361d-f672-425c-a7de-506bbac8167d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            " \n",
            "b\n",
            "o\n",
            "u\n",
            "g\n",
            "h\n",
            "t\n",
            " \n",
            "m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "KelaTlvK6h9-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "RMDdjXLC6mBB",
        "outputId": "7307ef1d-167d-4f9d-e750-0ad55ef93655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'I' b' ' b'b' b'o' b'u' b'g' b'h' b't' b' ' b'm' b'y' b' ' b'f' b'i'\n",
            " b'r' b's' b't' b' ' b'k' b'e' b'y' b' ' b'f' b'r' b'o' b'm' b' ' b'm'\n",
            " b'y' b' ' b'b' b'a' b'b' b'y' b' ' b'm' b'o' b'm' b'm' b'a' b' ' b'b'\n",
            " b'r' b'o' b't' b'h' b'e' b'r' b'\\r' b'\\n' b'I' b' ' b'b' b'o' b'u' b'g'\n",
            " b'h' b't' b' ' b'm' b'y' b' ' b'f' b'i' b'r' b's' b't' b' ' b'k' b'e'\n",
            " b'y' b'\\r' b'\\n' b'B' b'o' b'u' b'g' b'h' b't' b' ' b'm' b'y' b' ' b'b'\n",
            " b'o' b'u' b'g' b'h' b't' b' ' b'm' b'y' b' ' b'f' b'i' b'r' b's' b't'\n",
            " b' ' b'k' b'e'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "id": "y3WsA8N36oaK",
        "outputId": "9daa6086-cb2a-4412-b321-96bc5bf532ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'I bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first ke'\n",
            "b'y\\r\\nI bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first'\n",
            "b' key\\r\\nYeah hustling on my city streets\\r\\nTrying to get a whole key i bought my frist key my first key\\r'\n",
            "b'\\nInand we was getting em like for twenty five\\r\\nColombian connect homey we was getting fly\\r\\nWe on the '\n",
            "b'grind and our nuts got bigga\\r\\nAnd every day we in the motherfucking hood our guns got bigga\\r\\nStunting'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "  "
      ],
      "metadata": {
        "id": "PnUWJKBB6sAt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "id": "NoggDMtp6t-B",
        "outputId": "985c1541-59e1-4fc5-9ff6-eb1f4294976f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "CWrOZNM36wK8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "id": "Dopw49EE6zHX",
        "outputId": "fc15ef44-1116-4abc-bf01-a1ee1da52d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'I bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first k'\n",
            "Target: b' bought my first key from my baby momma brother\\r\\nI bought my first key\\r\\nBought my bought my first ke'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "oXPAFqEv62sc",
        "outputId": "d73899c7-ca89-4871-b582-25f309ea9de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "icC0Yk-e66i5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, rnn_units, vocab_size, embedding_dim):\n",
        "    super().__init__(self)\n",
        "    self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru=tf.keras.layers.GRU(rnn_units,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True)\n",
        "    self.dense=tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x=inputs\n",
        "    x=self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x=self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "    "
      ],
      "metadata": {
        "id": "DECQdHVM7UFb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "lM-hNMiu-T8r"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}